# save as myopia_diagnosis_complete_final.py
import pandas as pd
import numpy as np
import json
import torch
import random
import os
import traceback
import gc
import hashlib
import time
import warnings
import matplotlib.pyplot as plt
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
from collections import defaultdict
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.preprocessing import LabelEncoder, StandardScaler, label_binarize
from sklearn.metrics import (classification_report, confusion_matrix, roc_curve, auc, 
                           accuracy_score, precision_score, recall_score, f1_score)
import joblib
from openpyxl import Workbook
from openpyxl.styles import Font, PatternFill, Alignment
from openpyxl.utils import get_column_letter
warnings.filterwarnings('ignore')

# Â∞ùËØïÂØºÂÖ• xgboostÔºåÂ¶ÇÊûúÂ§±Ë¥•ÂàôËÆæ‰∏∫ÂèØÈÄâ
try:
    from xgboost import XGBClassifier
    XGBOOST_AVAILABLE = True
    print("‚úÖ XGBoost available")
except ImportError:
    XGBOOST_AVAILABLE = False
    print("‚ö†Ô∏è XGBoost not installed, skipping XGBoost model")

def set_random_seeds(seed=42):
    """ËÆæÁΩÆÊâÄÊúâÈöèÊú∫ÁßçÂ≠ê‰øùËØÅÂèØÈáçÂ§çÊÄß"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    print(f"‚úÖ Random seeds set to: {seed}")

# Ë∞ÉÁî®ËÆæÁΩÆÈöèÊú∫ÁßçÂ≠ê
set_random_seeds(42)

# Ê£ÄÊü•ÊòØÂê¶ÊúâGPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device: {device}")

class PatientIDGenerator:
    """Patient ID Generator (generates 16-bit unique identifier)"""
    
    @staticmethod
    def generate_patient_id(patient_info: Dict[str, Any]) -> str:
        """
        Generate a 16-bit unique identifier similar to Docker container ID
        Format: 4 groups of 4 hex digits, e.g.: a1b2-c3d4-e5f6-g7h8
        """
        # Create unique string
        unique_str = f"{patient_info.get('ÂßìÂêç', '')}_{patient_info.get('ÊÄßÂà´', '')}_{patient_info.get('Âπ¥ÈæÑ', '')}_{time.time_ns()}"
        
        # Generate unique value using SHA256 hash
        hash_obj = hashlib.sha256(unique_str.encode())
        hex_digest = hash_obj.hexdigest()[:16]  # Take first 16 characters
        
        # Format into 4 groups of 4
        formatted_id = '-'.join([hex_digest[i:i+4] for i in range(0, 16, 4)])
        
        return formatted_id.upper()

class ModelPerformanceTracker:
    """Model Performance Tracker"""
    
    def __init__(self):
        self.history = []
        self.best_models = {}
        self.current_iteration = 0
        
    def record_performance(self, model_name: str, metrics: Dict, iteration: int):
        """Record model performance"""
        record = {
            'model_name': model_name,
            'iteration': iteration,
            'timestamp': datetime.now().isoformat(),
            **metrics
        }
        self.history.append(record)
        
        # Update best model
        if model_name not in self.best_models or metrics['accuracy'] > self.best_models[model_name]['metrics']['accuracy']:
            self.best_models[model_name] = {
                'iteration': iteration,
                'metrics': metrics,
                'timestamp': record['timestamp']
            }
            
    def get_best_models(self) -> Dict:
        """Get best model information"""
        return self.best_models
    
    def save_history(self, filepath: str = "model_history.json"):
        """Save history"""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.history, f, ensure_ascii=False, indent=2)
            
    def load_history(self, filepath: str = "model_history.json"):
        """Load history"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                self.history = json.load(f)
        except FileNotFoundError:
            print(f"History file {filepath} not found")

class MyopiaRuleClassifier:
    """Rule-based Diagnosis System"""
    AXIAL_LENGTH_THRESHOLD = 26.00
    AL_CR_CONSTANT = 337.5

    def __init__(self, use_cycloplegic: bool = False):
        self.use_cycloplegic = use_cycloplegic
        self._load_medical_reference_data()

    def _load_medical_reference_data(self):
        self.axial_ref = pd.DataFrame({
            'age': [6,7,8,9,10,11,12,13,14,15],
            'min_len': [20.93,21.07,21.30,21.45,21.60,21.71,21.79,22.07,21.92,22.10],
            'max_len': [23.98,24.04,24.27,24.46,24.67,24.80,24.84,24.65,24.82,24.68],
            'mean': [22.46,22.68,22.90,23.05,23.22,23.38,23.52,23.62,23.72,23.39]
        })
        self.corneal_ref = pd.DataFrame({
            'age': [6,7,8,9,10,11,12,13,14,15],
            'min_curv': [7.93,7.09,7.42,7.41,7.41,7.42,7.39,7.39,7.36,7.40],
            'max_curv': [8.45,8.70,8.41,8.43,8.43,8.41,8.43,8.46,8.47,8.38]
        })
        self.vision_ref = {3:0.5,4:0.6,5:0.8,6:1.0}

    def _check_axial_length(self, age: int, al: float) -> str:
        ref = self.axial_ref[self.axial_ref['age'] == age]
        if ref.empty:
            return "Age out of reference range"
        min_len = ref['min_len'].values[0]
        max_len = ref['max_len'].values[0]
        if al < min_len:
            return f"Axial length too short (below {min_len:.2f}mm)"
        elif al > max_len:
            return f"Axial length too long (above {max_len:.2f}mm)"
        else:
            return f"Normal range ({min_len:.2f}-{max_len:.2f}mm)"

    def calculate_al_cr_ratio(self, al: float, corneal_curv: float) -> float:
        return round((al * self.jiaomoqulv(corneal_curv)) / self.AL_CR_CONSTANT, 2)

    def jiaomoqulv(self, curv: float) -> float:
        return self.AL_CR_CONSTANT / curv

    def _check_corneal_curvature(self, age: int, corneal_curv: float) -> str:
        ref = self.corneal_ref[self.corneal_ref['age'] == age]
        if ref.empty:
            return "Age out of reference range"
        min_curv = ref['min_curv'].values[0]
        max_curv = ref['max_curv'].values[0]
        if corneal_curv < min_curv:
            return f"Corneal curvature too steep (below {min_curv:.2f}D)"
        elif corneal_curv > max_curv:
            return f"Corneal curvature too flat (above {max_curv:.2f}D)"
        else:
            return f"Normal range ({min_curv:.2f}-{max_curv:.2f}D)"

    def _check_vision(self, age: int, va: float) -> str:
        if age in self.vision_ref:
            standard_va = self.vision_ref[age]
            if va < standard_va:
                return f"Vision development lag (age {age} standard: {standard_va}, current: {va})"
            else:
                return f"Normal vision (meets age {age} standard)"
        else:
            return "Age out of vision reference range"

    def diagnose(self, patient_data: Dict[str, float]) -> Dict:
        required_fields = ['age','se','al','corneal_curv','va','is_cycloplegic']
        if any(field not in patient_data for field in required_fields):
            missing = [f for f in required_fields if f not in patient_data]
            raise ValueError(f"Missing required fields: {missing}")
        
        diagnosis = {
            'axial_check': self._check_axial_length(patient_data['age'], patient_data['al']),
            'corneal_curv_check': self._check_corneal_curvature(patient_data['age'], patient_data['corneal_curv']),
            'vision_check': self._check_vision(patient_data['age'], patient_data['va']),
            'al_cr_ratio': self.calculate_al_cr_ratio(patient_data['al'], patient_data['corneal_curv']),
        }
        
        se = patient_data['se']
        age = patient_data['age']
        al = patient_data['al']
        corneal_curv = patient_data['corneal_curv']
        
        # Get reference values for age
        axial_ref = self.axial_ref[self.axial_ref['age'] == age]
        corneal_ref = self.corneal_ref[self.corneal_ref['age'] == age]
        
        # Calculate deviation indicators
        if not axial_ref.empty:
            al_deviation = (al - axial_ref['mean'].values[0])/axial_ref['mean'].values[0] * 100
        else:
            al_deviation = 0
            
        if not corneal_ref.empty:
            corneal_mean = (corneal_ref['min_curv'].values[0] + corneal_ref['max_curv'].values[0]) / 2
            corneal_deviation = corneal_curv - corneal_mean
        else:
            corneal_deviation = 0
        
        # Adjusted diagnosis logic - return English diagnoses directly
        se_threshold = -0.50 if patient_data['is_cycloplegic'] else -0.75
        if se >= 0.75:
            diagnosis['stage'] = "Hyperopia"  # ËøúËßÜ
        elif 0.75 > se >= se_threshold:
            # Emmetropia needs to consider axial length/corneal curvature deviation
            if abs(al_deviation) < 5 and abs(corneal_deviation) < 0.2:
                diagnosis['stage'] = "Emmetropia"  # Ê≠£ËßÜ
            else:
                diagnosis['stage'] = "Pre-myopia"  # ËøëËßÜÂâçÊúüÔºà‰øÆÊîπÔºöÁñë‰ººËøëËßÜÂâçÊúü -> ËøëËßÜÂâçÊúüÔºâ
        elif se_threshold > se >= -3.25:
            diagnosis['stage'] = "Mild Myopia" if al_deviation < 10 else "Mild Myopia (Long Axial)"
        elif -3.25 > se >= -6.00:
            diagnosis['stage'] = "Moderate Myopia"
        elif se < -6.00 or (al >= self.AXIAL_LENGTH_THRESHOLD and al_deviation >= 15):
            diagnosis['stage'] = "High Myopia"
            diagnosis['warning'] = "Need to be alert to pathological myopia risk"
        else:
            diagnosis['stage'] = "Unclassified"
        
        return diagnosis

class AutoRepairingModel:
    """Auto-repairing model (automatically adjusts when performance declines)"""
    
    def __init__(self, base_model, model_name: str):
        self.base_model = base_model
        self.model_name = model_name
        self.performance_history = []
        self.fail_count = 0
        self.repair_count = 0
        
    def train_with_repair(self, X_train, y_train, X_val, y_val, max_retries=3):
        """Training with repair functionality"""
        for attempt in range(max_retries):
            try:
                # Train model
                self.base_model.fit(X_train, y_train)
                
                # Validate performance
                y_pred = self.base_model.predict(X_val)
                accuracy = accuracy_score(y_val, y_pred)
                
                # Check for overfitting
                if len(self.performance_history) >= 2:
                    last_accuracy = self.performance_history[-1]['accuracy']
                    if accuracy < last_accuracy * 0.8:  # Performance drop > 20%
                        print(f"  ‚ö†Ô∏è {self.model_name} performance drop ({accuracy:.2%} < {last_accuracy:.2%}), attempting repair...")
                        self._apply_repair()
                        continue
                
                # Record performance
                self.performance_history.append({
                    'attempt': attempt,
                    'accuracy': accuracy,
                    'timestamp': datetime.now().isoformat()
                })
                
                return accuracy
                
            except Exception as e:
                self.fail_count += 1
                print(f"  ‚ùå {self.model_name} training failed (attempt {attempt+1}/{max_retries}): {e}")
                
                if attempt < max_retries - 1:
                    self._apply_repair()
                else:
                    raise
        
        return 0.0
    
    def _apply_repair(self):
        """Apply repair strategy"""
        self.repair_count += 1
        
        if self.model_name == 'RandomForest':
            # RandomForest repair: increase tree depth or number
            if hasattr(self.base_model, 'n_estimators'):
                self.base_model.n_estimators = min(self.base_model.n_estimators * 2, 200)
            if hasattr(self.base_model, 'max_depth'):
                self.base_model.max_depth = self.base_model.max_depth + 2 if self.base_model.max_depth else 10
                
        elif self.model_name == 'XGBoost' and XGBOOST_AVAILABLE:
            # XGBoost repair: adjust learning rate
            if hasattr(self.base_model, 'learning_rate'):
                self.base_model.learning_rate = max(self.base_model.learning_rate * 0.8, 0.01)
        
        elif self.model_name == 'LogisticRegression':
            # LogisticRegression repair: increase iterations
            if hasattr(self.base_model, 'max_iter'):
                self.base_model.max_iter = min(self.base_model.max_iter * 2, 5000)
        
        elif self.model_name == 'SVM':
            # SVM repair: adjust C parameter
            if hasattr(self.base_model, 'C'):
                self.base_model.C = self.base_model.C * 1.5
                
        elif self.model_name == 'GradientBoosting':
            # GradientBoosting repair: adjust learning rate
            if hasattr(self.base_model, 'learning_rate'):
                self.base_model.learning_rate = max(self.base_model.learning_rate * 0.8, 0.01)
                
        print(f"  üîß {self.model_name} applied repair strategy #{self.repair_count}")

class MultiModelTraining:
    """Multi-model training and comparison"""
    
    def __init__(self):
        self.models = {}
        self.scaler = StandardScaler()
        self.feature_columns = None
        self.performance_tracker = ModelPerformanceTracker()
        
    def prepare_features(self, data_df):
        """Prepare features"""
        print("  Preparing features...")
        
        # Basic features
        age_se_mean = data_df.groupby('Âπ¥ÈæÑ')['È™åÂÖâ'].mean().to_dict()
        features = []
        
        for _, row in data_df.iterrows():
            # Replace direct refraction value with difference from age group mean
            se_diff = row['È™åÂÖâ'] - age_se_mean.get(row['Âπ¥ÈæÑ'], row['È™åÂÖâ'])
            feature_vector = [
                row['Âπ¥ÈæÑ'],
                1 if row['ÊÄßÂà´'] == 'Â•≥' else 0,
                row['ËßÜÂäõ'],
                se_diff,
                row['ÁúºËΩ¥'],
                row['ËΩ¥ÁéáÊØî'],
                row['ËßíËÜúÊõ≤Áéá'],
                row['ÁúºËΩ¥'] * (337.5 / row['ËßíËÜúÊõ≤Áéá']) / 337.5,
                abs(se_diff)
            ]
            features.append(feature_vector)
        
        # Update feature column names
        self.feature_columns = ['Age', 'Gender_Female', 'Vision', 'Refraction_AgeDiff', 'Axial_Length', 
                              'Axial_Ratio', 'Corneal_Curvature', 'AL_CR_Ratio', 'Refraction_Diff_Abs']
        
        features_array = np.array(features)
        
        # Standardize features
        if len(features_array) > 1:
            features_array = self.scaler.fit_transform(features_array)
        
        return features_array
    
    def initialize_models(self):
        """Initialize all models"""
        print("  Initializing machine learning models...")
        
        # 1. Random Forest
        self.models['RandomForest'] = AutoRepairingModel(
            RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42,
                class_weight='balanced'
            ),
            'RandomForest'
        )
        
        # 2. Logistic Regression
        self.models['LogisticRegression'] = AutoRepairingModel(
            LogisticRegression(
                max_iter=1000,
                random_state=42,
                multi_class='ovr',
                class_weight='balanced'
            ),
            'LogisticRegression'
        )
        
        # 3. Support Vector Machine
        self.models['SVM'] = AutoRepairingModel(
            SVC(
                kernel='rbf',
                probability=True,
                random_state=42,
                class_weight='balanced'
            ),
            'SVM'
        )
        
        # 4. Gradient Boosting
        self.models['GradientBoosting'] = AutoRepairingModel(
            GradientBoostingClassifier(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=5,
                random_state=42
            ),
            'GradientBoosting'
        )
        
        # 5. XGBoost (optional)
        if XGBOOST_AVAILABLE:
            self.models['XGBoost'] = AutoRepairingModel(
                XGBClassifier(
                    n_estimators=100,
                    learning_rate=0.1,
                    max_depth=5,
                    random_state=42,
                    use_label_encoder=False,
                    eval_metric='mlogloss'
                ),
                'XGBoost'
            )
        else:
            print("  ‚ö†Ô∏è Skipping XGBoost model (not installed)")
        
        print(f"  Initialized {len(self.models)} models: {list(self.models.keys())}")
    
    def train_and_evaluate_all(self, train_data, val_data, test_data, iteration=1):
        """Train and evaluate all models"""
        print(f"\n  --- Training iteration {iteration} ---")
        #for _ in range():
        
        # Prepare data
        X_train = self.prepare_features(train_data)
        y_train = train_data['ËØäÊñ≠ÁºñÁ†Å'].values
        
        X_val = self.prepare_features(val_data)
        y_val = val_data['ËØäÊñ≠ÁºñÁ†Å'].values
        
        X_test = self.prepare_features(test_data)
        y_test = test_data['ËØäÊñ≠ÁºñÁ†Å'].values
        
        results = {}
        
        for model_name, model_wrapper in self.models.items():
            print(f"\n  Training {model_name}...")
            
            try:
                # Train with repair functionality
                start_time = time.time()
                val_accuracy = model_wrapper.train_with_repair(X_train, y_train, X_val, y_val)
                training_time = time.time() - start_time
                
                # Test performance
                model = model_wrapper.base_model
                y_pred = model.predict(X_test)
                
                # Check if predict_proba method exists
                has_proba = hasattr(model, 'predict_proba')
                y_proba = model.predict_proba(X_test) if has_proba else None
                
                # Calculate various metrics
                metrics = {
                    'accuracy': accuracy_score(y_test, y_pred),
                    'precision': precision_score(y_test, y_pred, average='weighted', zero_division=0),
                    'recall': recall_score(y_test, y_pred, average='weighted', zero_division=0),
                    'f1_score': f1_score(y_test, y_pred, average='weighted', zero_division=0),
                    'val_accuracy': val_accuracy,
                    'training_time': training_time,
                    'repair_count': model_wrapper.repair_count,
                    'fail_count': model_wrapper.fail_count,
                    'has_proba': has_proba
                }
                
                # Record performance
                self.performance_tracker.record_performance(model_name, metrics, iteration)
                
                results[model_name] = {
                    'model': model,
                    'metrics': metrics,
                    'predictions': y_pred,
                    'probabilities': y_proba,
                    'y_test': y_test,
                    'X_test': X_test
                }
                
                print(f"    Accuracy: {metrics['accuracy']:.2%}")
                print(f"    F1 Score: {metrics['f1_score']:.2%}")
                print(f"    Training time: {training_time:.2f}s")
                print(f"    Repair count: {model_wrapper.repair_count}")
                
            except Exception as e:
                print(f"    ‚ùå {model_name} training failed: {e}")
                results[model_name] = {'error': str(e)}
        
        return results
    
    def compare_models(self, results):
        """Compare model performance"""
        print("\n  --- Model Performance Comparison ---")
        
        performance_table = []
        for model_name, result in results.items():
            if 'metrics' in result:
                metrics = result['metrics']
                performance_table.append({
                    'Model': model_name,
                    'Accuracy': f"{metrics['accuracy']:.2%}",
                    'F1-Score': f"{metrics['f1_score']:.2%}",
                    'Training Time': f"{metrics['training_time']:.2f}s",
                    'Repairs': metrics['repair_count']
                })
        
        # Create comparison table
        comparison_df = pd.DataFrame(performance_table)
        if not comparison_df.empty:
            print(comparison_df.to_string(index=False))
            
            # Find best model
            best_model = max(performance_table, key=lambda x: float(x['Accuracy'].rstrip('%')))
            print(f"\n  üèÜ Best model: {best_model['Model']} (Accuracy: {best_model['Accuracy']})")
        else:
            print("    No comparable model results")
        
        return comparison_df
    
    def ensemble_predict(self, results, X):
        """Ensemble prediction (using average probabilities of all models)"""
        print("\n  Executing ensemble prediction...")
        
        probabilities_list = []
        model_count = 0
        
        for model_name, result in results.items():
            if 'probabilities' in result and result['probabilities'] is not None:
                probabilities_list.append(result['probabilities'])
                model_count += 1
        
        if probabilities_list:
            # Average probabilities of all models
            avg_probabilities = np.mean(probabilities_list, axis=0)
            ensemble_predictions = np.argmax(avg_probabilities, axis=1)
            
            print(f"    Used {model_count} models for ensemble")
            return ensemble_predictions, avg_probabilities
        else:
            print("    ‚ö†Ô∏è No models support probability prediction, cannot perform ensemble")
            return None, None
    
    def analyze_roc_for_all_models(self, results, label_encoder, output_dir="roc_analysis"):
        """Analyze ROC curves and AUC values for all models"""
        print(f"\n  --- Analyzing ROC curves and AUC for all models ---")
        
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        roc_results = {}
        
        for model_name, result in results.items():
            if 'probabilities' not in result or result['probabilities'] is None:
                print(f"    ‚ö†Ô∏è {model_name} does not support probability prediction, skipping ROC analysis")
                continue
            
            if 'y_test' not in result:
                print(f"    ‚ö†Ô∏è {model_name} has no test data, skipping ROC analysis")
                continue
            
            try:
                print(f"    üìä Analyzing ROC curve for {model_name}...")
                
                # Get test data and probabilities
                y_test = result['y_test']
                y_proba = result['probabilities']
                
                # Get number of classes
                n_classes = y_proba.shape[1]
                
                # Get class names
                if label_encoder is not None and hasattr(label_encoder, 'classes_'):
                    # Get original diagnosis names
                    original_class_names = label_encoder.classes_
                    
                    # Create Chinese-English mapping dictionary
                    diagnosis_translation = {
                        # English to English (they should already be in English from the rule classifier)
                        "Hyperopia": "Hyperopia",
                        "Emmetropia": "Emmetropia", 
                        "Pre-myopia": "Pre-myopia",  # ‰øÆÊîπÔºöËøëËßÜÂâçÊúü
                        "Mild Myopia": "Mild Myopia",
                        "Mild Myopia (Long Axial)": "Mild Myopia (Long Axial)",
                        "Moderate Myopia": "Moderate Myopia",
                        "High Myopia": "High Myopia",
                        "Unclassified": "Unclassified",
                        "Diagnosis Failed": "Diagnosis Failed"
                    }
                    
                    # Translate Chinese class names to English
                    class_names = []
                    for name in original_class_names:
                        # Since our rule classifier now returns English directly,
                        # we can just use the original names
                        if isinstance(name, str):
                            class_names.append(name)
                        else:
                            class_names.append(str(name))
                else:
                    # If no label encoder, use default Class numbering
                    class_names = [f'Class {i}' for i in range(n_classes)]
                
                # Calculate ROC curve and AUC for each class
                fpr = {}
                tpr = {}
                roc_auc = {}
                
                # Binarize labels for multi-class ROC
                y_test_bin = label_binarize(y_test, classes=range(n_classes))
                
                for i in range(n_classes):
                    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_proba[:, i])
                    roc_auc[i] = auc(fpr[i], tpr[i])
                
                # Calculate micro-average ROC
                fpr["micro"], tpr["micro"], _ = roc_curve(y_test_bin.ravel(), y_proba.ravel())
                roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
                
                # Calculate macro-average ROC
                all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))
                mean_tpr = np.zeros_like(all_fpr)
                for i in range(n_classes):
                    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
                mean_tpr /= n_classes
                fpr["macro"] = all_fpr
                tpr["macro"] = mean_tpr
                roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])
                
                # Plot ROC curve
                plt.figure(figsize=(12, 8))
                colors = plt.cm.rainbow(np.linspace(0, 1, n_classes))
                
                # Plot ROC curve for each class
                for i, color in zip(range(n_classes), colors):
                    if i < len(class_names):
                        label_name = class_names[i]
                    else:
                        label_name = f'Class {i}'
                    
                    plt.plot(fpr[i], tpr[i], color=color, lw=2,
                            label=f'{label_name} (AUC = {roc_auc[i]:.3f})')
                
                # Plot average ROC curves
                plt.plot(fpr["micro"], tpr["micro"],
                        label=f'Micro-average (AUC = {roc_auc["micro"]:.3f})',
                        color='deeppink', linestyle=':', linewidth=4)
                
                plt.plot(fpr["macro"], tpr["macro"],
                        label=f'Macro-average (AUC = {roc_auc["macro"]:.3f})',
                        color='navy', linestyle=':', linewidth=4)
                
                plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random (AUC = 0.5)')
                
                plt.xlim([0.0, 1.0])
                plt.ylim([0.0, 1.05])
                plt.xlabel('False Positive Rate (FPR)', fontsize=12)
                plt.ylabel('True Positive Rate (TPR)', fontsize=12)
                plt.title(f'{model_name} - Multi-class ROC Curve', fontsize=14, fontweight='bold')
                plt.legend(loc="lower right", fontsize=9)
                plt.grid(True, alpha=0.3)
                
                # Save image
                roc_file = os.path.join(output_dir, f"{model_name}_ROC_Curve.png")
                plt.tight_layout()
                plt.savefig(roc_file, dpi=300, bbox_inches='tight')
                plt.close()
                
                print(f"      ROC curve saved to: {roc_file}")
                
                # Save AUC results
                roc_results[model_name] = {
                    'class_auc': roc_auc,
                    'class_names_english': class_names,
                    'micro_auc': roc_auc['micro'],
                    'macro_auc': roc_auc['macro']
                }
                
                # Print AUC summary
                print(f"      AUC Summary:")
                print(f"        Micro-average AUC: {roc_auc['micro']:.3f}")
                print(f"        Macro-average AUC: {roc_auc['macro']:.3f}")
                for i in range(n_classes):
                    if i < len(class_names):
                        label_name = class_names[i]
                    else:
                        label_name = f'Class {i}'
                    print(f"        {label_name} AUC: {roc_auc[i]:.3f}")
                    
            except Exception as e:
                print(f"    ‚ùå ROC analysis failed for {model_name}: {e}")
                roc_results[model_name] = {'error': str(e)}
        
        # Save all ROC results
        roc_summary_file = os.path.join(output_dir, "ROC_AUC_Summary.json")
        with open(roc_summary_file, 'w', encoding='utf-8') as f:
            # Convert numpy types to Python types
            serializable_results = {}
            for model_name, result in roc_results.items():
                if 'error' in result:
                    serializable_results[model_name] = result
                else:
                    serializable_results[model_name] = {
                        'micro_auc': float(result['micro_auc']),
                        'macro_auc': float(result['macro_auc']),
                        'class_names_english': result.get('class_names_english', []),
                        'class_auc': {str(k): float(v) for k, v in result['class_auc'].items() if k not in ['micro', 'macro']}
                    }
            
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
        
        print(f"\n  ‚úÖ ROC analysis completed, results saved to: {output_dir}")
        return roc_results
    
    def save_all_models(self, base_dir="saved_models"):
        """Save all models"""
        if not os.path.exists(base_dir):
            os.makedirs(base_dir)
        
        for model_name, model_wrapper in self.models.items():
            try:
                model_path = os.path.join(base_dir, f"{model_name}_model.joblib")
                joblib.dump(model_wrapper.base_model, model_path)
                print(f"  Saved {model_name} model")
            except Exception as e:
                print(f"  Failed to save {model_name}: {e}")
        
        # Save feature processor
        scaler_path = os.path.join(base_dir, "scaler.joblib")
        joblib.dump(self.scaler, scaler_path)
        
        # Save performance history
        self.performance_tracker.save_history(os.path.join(base_dir, "model_history.json"))
        
        # Save label encoder (if available)
        label_encoder_path = os.path.join(base_dir, "label_encoder.joblib")
        if hasattr(self, 'label_encoder') and self.label_encoder is not None:
            joblib.dump(self.label_encoder, label_encoder_path)
            print(f"  Saved label encoder")
        
        print(f"\n  All models saved to: {base_dir}")

class EnhancedDataProcessor:
    """Enhanced Data Processor"""
    
    def __init__(self):
        self.rule_classifier = MyopiaRuleClassifier()
        self.label_encoder = LabelEncoder()
        self.patient_id_map = {}  # Mapping: original info -> patient ID
        
    def load_data(self, excel_path: str):
        print("1. Loading data...")
        df = pd.read_excel(excel_path, header=None)  # Read without headers
        print(f"  Raw data: {len(df)} records")
        print(f"  Number of columns: {len(df.columns)}")
        return df
    
    def process_data(self, df):
        """Process data"""
        print("2. Processing data...")
        
        processed_data = []
        
        for idx, row in df.iterrows():
            # Check if necessary columns exist
            if len(row) < 11:
                print(f"  Row {idx+1} incomplete, skipping")
                continue
            
            # Process right eye
            if not pd.isna(row[3]) and not pd.isna(row[5]) and not pd.isna(row[7]) and not pd.isna(row[9]):
                try:
                    eye_data = self._process_single_eye(row, 'right')
                    processed_data.append(eye_data)
                except Exception as e:
                    print(f"  Failed to process right eye data (row {idx+1}): {e}")
            
            # Process left eye
            if not pd.isna(row[4]) and not pd.isna(row[6]) and not pd.isna(row[8]) and not pd.isna(row[10]):
                try:
                    eye_data = self._process_single_eye(row, 'left')
                    processed_data.append(eye_data)
                except Exception as e:
                    print(f"  Failed to process left eye data (row {idx+1}): {e}")
        
        data_df = pd.DataFrame(processed_data)
        print(f"  After processing: {len(data_df)} eye data points")
        return data_df
    
    def _process_single_eye(self, row, eye_side: str) -> Dict:
        """Process single eye data"""
        # Get data based on Excel column positions
        if eye_side == 'right':
            va = float(row[3])
            se_str = str(row[5])
            al = float(row[7])
            axial_ratio = float(row[9])
        else:
            va = float(row[4])
            se_str = str(row[6])
            al = float(row[8])
            axial_ratio = float(row[10])
        
        age_str = str(row[2])
        if '/' in age_str:
            years, _ = age_str.split('/')
            age = int(years)
        else:
            age = int(age_str)
        
        # Process refraction string
        se_str_clean = str(se_str).strip()
        if se_str_clean.startswith('+'):
            se = float(se_str_clean[1:])
        elif se_str_clean.startswith('-'):
            se = float(se_str_clean)
        else:
            se = float(se_str_clean)
        
        # Calculate corneal curvature
        corneal_curv = 45 - (axial_ratio - 3.0) * 10
        if corneal_curv < 38:
            corneal_curv = 38
        elif corneal_curv > 48:
            corneal_curv = 48
        
        # Create patient key
        gender_num = row[1] if len(row) > 1 else 0
        patient_key = f"{row[0]}_{gender_num}_{age_str}"
        
        return {
            'ÂßìÂêç': str(row[0]),
            'ÊÄßÂà´': 'Â•≥' if gender_num == 1 else 'Áî∑',
            'Âπ¥ÈæÑ': age,
            'ÁúºÂà´': 'Âè≥Áúº' if eye_side == 'right' else 'Â∑¶Áúº',
            'ËßÜÂäõ': va,
            'È™åÂÖâ': se,
            'ÁúºËΩ¥': al,
            'ËΩ¥ÁéáÊØî': axial_ratio,
            'ËßíËÜúÊõ≤Áéá': round(corneal_curv, 2),
            'ÂéüÂßã_Âπ¥ÈæÑ': age_str,
            'ÂéüÂßã_È™åÂÖâ': se_str_clean,
            'patient_key': patient_key
        }
    
    def generate_patient_ids(self, data_df):
        """Generate unique IDs for each patient"""
        print("3. Generating unique patient IDs...")
        
        unique_patients = data_df['patient_key'].unique()
        
        for patient_key in unique_patients:
            parts = patient_key.split('_')
            if len(parts) >= 3:
                patient_info = {
                    'ÂßìÂêç': parts[0],
                    'ÊÄßÂà´': parts[1],
                    'Âπ¥ÈæÑ': parts[2]
                }
                
                patient_id = PatientIDGenerator.generate_patient_id(patient_info)
                self.patient_id_map[patient_key] = patient_id
        
        # Add patient IDs to dataframe
        data_df['patient_id'] = data_df['patient_key'].map(self.patient_id_map)
        
        print(f"  Generated unique IDs for {len(unique_patients)} patients")
        print(f"  Example IDs: {list(self.patient_id_map.values())[:3]}")
        
        return data_df
    
    def run_rule_diagnosis(self, data_df):
        """Run rule-based diagnosis"""
        print("4. Generating rule-based diagnosis labels...")
        
        diagnoses = []
        for _, row in data_df.iterrows():
            try:
                patient_data = {
                    'age': row['Âπ¥ÈæÑ'],
                    'se': row['È™åÂÖâ'],
                    'al': row['ÁúºËΩ¥'],
                    'corneal_curv': row['ËßíËÜúÊõ≤Áéá'],
                    'va': row['ËßÜÂäõ'],
                    'is_cycloplegic': False
                }
                diagnosis = self.rule_classifier.diagnose(patient_data)
                diagnoses.append(diagnosis['stage'])
            except Exception as e:
                print(f"  Diagnosis failed (patient: {row['ÂßìÂêç']}, eye: {row['ÁúºÂà´']}): {e}")
                diagnoses.append("Diagnosis Failed")
        
        data_df['ËØäÊñ≠ÁªìÊûú'] = diagnoses
        
        # Encode diagnosis results
        valid_diagnoses = [d for d in diagnoses if d != "Diagnosis Failed"]
        if valid_diagnoses:
            self.label_encoder.fit(valid_diagnoses)
            data_df['ËØäÊñ≠ÁºñÁ†Å'] = data_df['ËØäÊñ≠ÁªìÊûú'].apply(
                lambda x: self.label_encoder.transform([x])[0] if x in self.label_encoder.classes_ else -1
            )
        else:
            data_df['ËØäÊñ≠ÁºñÁ†Å'] = -1
        
        print(f"  Diagnosis category distribution:")
        for label, count in data_df['ËØäÊñ≠ÁªìÊûú'].value_counts().items():
            print(f"    {label}: {count} ({count/len(data_df):.1%})")
        
        return data_df
    
    # ‰øÆÊîπ EnhancedDataProcessor Á±ª‰∏≠ÁöÑ split_data_by_patient_id ÊñπÊ≥ïÔºö

def split_data_by_patient_id(self, data_df, train_size=0.7, val_size=0.15, test_size=0.15, random_state=None):
    """ÊåâÊÇ£ËÄÖIDÂàÜÂâ≤Êï∞ÊçÆÔºàÊ∑ªÂä†ÈöèÊú∫ÁßçÂ≠êÂèÇÊï∞Ôºâ"""
    print("5. ÊåâÊÇ£ËÄÖIDÂàÜÂâ≤Êï∞ÊçÆ...")
    
    # ËøáÊª§ÊéâËØäÊñ≠Â§±Ë¥•ÁöÑÊï∞ÊçÆ
    valid_data = data_df[data_df['ËØäÊñ≠ÁºñÁ†Å'] != -1].copy()
    if len(valid_data) == 0:
        raise ValueError("Ê≤°ÊúâÊúâÊïàÁöÑËØäÊñ≠Êï∞ÊçÆÂèØÁî®‰∫éÂàÜÂâ≤")
    
    unique_patients = valid_data['patient_id'].unique()
    
    # ÂàÜÂâ≤ÊÇ£ËÄÖID - ‰ΩøÁî®‰º†ÂÖ•ÁöÑÈöèÊú∫ÁßçÂ≠ê
    train_patients, temp_patients = train_test_split(
        unique_patients,
        test_size=(val_size + test_size),
        random_state=random_state  # ‰ΩøÁî®‰º†ÂÖ•ÁöÑÈöèÊú∫ÁßçÂ≠ê
    )
    
    val_relative_size = val_size / (val_size + test_size)
    val_patients, test_patients = train_test_split(
        temp_patients,
        test_size=(1 - val_relative_size),
        random_state=random_state  # ‰ΩøÁî®Áõ∏ÂêåÁöÑÈöèÊú∫ÁßçÂ≠êÁ°Æ‰øù‰∏ÄËá¥ÊÄß
    )
    
    # Ê†πÊçÆÊÇ£ËÄÖIDÂàÜÈÖçÊï∞ÊçÆ
    train_data = valid_data[valid_data['patient_id'].isin(train_patients)].copy()
    val_data = valid_data[valid_data['patient_id'].isin(val_patients)].copy()
    test_data = valid_data[valid_data['patient_id'].isin(test_patients)].copy()
    
    print(f"  ËÆ≠ÁªÉÈõÜ: {len(train_data)} Ê†∑Êú¨ ({len(train_data)/len(valid_data):.1%})")
    print(f"  È™åËØÅÈõÜ: {len(val_data)} Ê†∑Êú¨ ({len(val_data)/len(valid_data):.1%})")
    print(f"  ÊµãËØïÈõÜ: {len(test_data)} Ê†∑Êú¨ ({len(test_data)/len(valid_data):.1%})")
    
    # Ê£ÄÊü•Á±ªÂà´ÂàÜÂ∏É‰ª•Á°Æ‰øùÂ§öÊ†∑ÊÄß
    self._check_class_distribution(train_data, "ËÆ≠ÁªÉÈõÜ")
    self._check_class_distribution(val_data, "È™åËØÅÈõÜ")
    self._check_class_distribution(test_data, "ÊµãËØïÈõÜ")
    
    return train_data, val_data, test_data

def _check_class_distribution(self, data, dataset_name):
    """Ê£ÄÊü•Êï∞ÊçÆÈõÜÁöÑÁ±ªÂà´ÂàÜÂ∏É"""
    if len(data) > 0:
        print(f"    {dataset_name}Á±ªÂà´ÂàÜÂ∏É:")
        total = len(data)
        for diagnosis in data['ËØäÊñ≠ÁªìÊûú'].value_counts().index:
            count = (data['ËØäÊñ≠ÁªìÊûú'] == diagnosis).sum()
            percentage = count / total * 100
            print(f"      {diagnosis}: {count} ({percentage:.1f}%)")
    
    def save_datasets(self, train_data, val_data, test_data):
        """Save datasets"""
        print("6. Saving datasets...")
        
        train_data.to_excel("ËÆ≠ÁªÉÈõÜ.xlsx", index=False)
        val_data.to_excel("È™åËØÅÈõÜ.xlsx", index=False)
        test_data.to_excel("ÊµãËØïÈõÜ.xlsx", index=False)
        
        print("  Datasets saved as Excel files")

class ExcelReportGenerator:
    """Excel Report Generator with Explanations"""
    
    @staticmethod
    def generate_training_report(history_file="model_history.json", 
                               iteration_file="iteration_results.json",
                               output_file="ËÆ≠ÁªÉÊä•Âëä.xlsx"):
        """Generate Excel training report with explanations"""
        print(f"\n  Generating Excel training report...")
        
        try:
            # Create Excel workbook
            wb = Workbook()
            
            # 1. Training History Sheet with Explanation
            if os.path.exists(history_file):
                with open(history_file, 'r', encoding='utf-8') as f:
                    history_data = json.load(f)
                
                ws1 = wb.active
                ws1.title = "ËÆ≠ÁªÉÂéÜÂè≤"
                
                # Add explanation at the top
                ws1.merge_cells('A1:I2')
                explanation_cell = ws1.cell(row=1, column=1, 
                    value="üìä ËÆ≠ÁªÉÂéÜÂè≤Ë°®ËØ¥ÊòéÔºö\n"
                    "Ê≠§Ë°®Ê†ºËÆ∞ÂΩï‰∫ÜÊØè‰∏™Ê®°ÂûãÂú®ÊØèÊ¨°ËÆ≠ÁªÉËø≠‰ª£‰∏≠ÁöÑËØ¶ÁªÜÊÄßËÉΩÊåáÊ†á„ÄÇ\n"
                    "‚Ä¢ Ê®°ÂûãÂêçÁß∞Ôºö‰ΩøÁî®ÁöÑÊú∫Âô®Â≠¶‰π†ÁÆóÊ≥ï\n"
                    "‚Ä¢ Ëø≠‰ª£Ê¨°Êï∞ÔºöËÆ≠ÁªÉËΩÆÊ¨°\n"
                    "‚Ä¢ Êó∂Èó¥Êà≥ÔºöËÆ≠ÁªÉÂÆåÊàêÊó∂Èó¥\n"
                    "‚Ä¢ ÂáÜÁ°ÆÁéáÔºöÊ®°ÂûãÂú®ÊµãËØïÈõÜ‰∏äÁöÑÈ¢ÑÊµãÂáÜÁ°ÆÁéá\n"
                    "‚Ä¢ F1ÂàÜÊï∞ÔºöÁªºÂêàËÄÉËôëÁ≤æÁ°ÆÁéáÂíåÂè¨ÂõûÁéáÁöÑÊåáÊ†á\n"
                    "‚Ä¢ Á≤æÁ°ÆÁéáÔºöÊ≠£Á°ÆÈ¢ÑÊµãÁöÑÊ≠£‰æãÂç†ÊâÄÊúâÈ¢ÑÊµã‰∏∫Ê≠£‰æãÁöÑÊØî‰æã\n"
                    "‚Ä¢ Âè¨ÂõûÁéáÔºöÊ≠£Á°ÆÈ¢ÑÊµãÁöÑÊ≠£‰æãÂç†ÊâÄÊúâÂÆûÈôÖÊ≠£‰æãÁöÑÊØî‰æã\n"
                    "‚Ä¢ ËÆ≠ÁªÉÊó∂Èó¥(Áßí)ÔºöÊ®°ÂûãËÆ≠ÁªÉËÄóÊó∂\n"
                    "‚Ä¢ ‰øÆÂ§çÊ¨°Êï∞ÔºöËá™‰øÆÂ§çÂäüËÉΩËß¶ÂèëÁöÑÊ¨°Êï∞")
                explanation_cell.alignment = Alignment(wrap_text=True, vertical='center')
                explanation_cell.font = Font(bold=True, size=11)
                explanation_cell.fill = PatternFill(start_color="E6E6FA", end_color="E6E6FA", fill_type="solid")
                
                # Write headers (starting from row 4)
                headers = ["Ê®°ÂûãÂêçÁß∞", "Ëø≠‰ª£Ê¨°Êï∞", "Êó∂Èó¥Êà≥", "ÂáÜÁ°ÆÁéá", "F1ÂàÜÊï∞", 
                          "Á≤æÁ°ÆÁéá", "Âè¨ÂõûÁéá", "ËÆ≠ÁªÉÊó∂Èó¥(Áßí)", "‰øÆÂ§çÊ¨°Êï∞"]
                for col, header in enumerate(headers, 1):
                    cell = ws1.cell(row=4, column=col, value=header)
                    cell.font = Font(bold=True)
                    cell.fill = PatternFill(start_color="CCCCCC", end_color="CCCCCC", fill_type="solid")
                    cell.alignment = Alignment(horizontal='center')
                
                # Write data (starting from row 5)
                row = 5
                for record in history_data:
                    ws1.cell(row=row, column=1, value=record.get('model_name', ''))
                    ws1.cell(row=row, column=2, value=record.get('iteration', 0))
                    ws1.cell(row=row, column=3, value=record.get('timestamp', ''))
                    ws1.cell(row=row, column=4, value=record.get('accuracy', 0))
                    ws1.cell(row=row, column=5, value=record.get('f1_score', 0))
                    ws1.cell(row=row, column=6, value=record.get('precision', 0))
                    ws1.cell(row=row, column=7, value=record.get('recall', 0))
                    ws1.cell(row=row, column=8, value=record.get('training_time', 0))
                    ws1.cell(row=row, column=9, value=record.get('repair_count', 0))
                    row += 1
                
                # Adjust column widths
                for column in ws1.columns:
                    max_length = 0
                    column_letter = get_column_letter(column[0].column)
                    for cell in column:
                        try:
                            if cell.value and len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 50)
                    ws1.column_dimensions[column_letter].width = adjusted_width
                
                # Format numbers
                for row in ws1.iter_rows(min_row=5, max_row=row-1, min_col=4, max_col=8):
                    for cell in row:
                        if isinstance(cell.value, (int, float)):
                            cell.number_format = '0.00%' if cell.column <= 7 else '0.00'
            else:
                print(f"    ‚ö†Ô∏è Training history file {history_file} not found")
            
            # 2. Iteration Results Sheet with Explanation
            if os.path.exists(iteration_file):
                with open(iteration_file, 'r', encoding='utf-8') as f:
                    iteration_data = json.load(f)
                
                ws2 = wb.create_sheet("Ëø≠‰ª£ÁªìÊûú")
                
                # Add explanation at the top
                ws2.merge_cells('A1:E2')
                explanation_cell = ws2.cell(row=1, column=1, 
                    value="üìà Ëø≠‰ª£ÁªìÊûúË°®ËØ¥ÊòéÔºö\n"
                    "Ê≠§Ë°®Ê†ºÊ±áÊÄª‰∫ÜÊØèÊ¨°ËÆ≠ÁªÉËø≠‰ª£‰∏≠ÊâÄÊúâÊ®°ÂûãÁöÑÊÄßËÉΩË°®Áé∞„ÄÇ\n"
                    "‚Ä¢ Ëø≠‰ª£Ê¨°Êï∞ÔºöËÆ≠ÁªÉËΩÆÊ¨°ÁºñÂè∑\n"
                    "‚Ä¢ Ê®°ÂûãÂêçÁß∞Ôºö‰ΩøÁî®ÁöÑÊú∫Âô®Â≠¶‰π†ÁÆóÊ≥ï\n"
                    "‚Ä¢ ÂáÜÁ°ÆÁéáÔºöÊ®°ÂûãÂú®ÊµãËØïÈõÜ‰∏äÁöÑÈ¢ÑÊµãÂáÜÁ°ÆÁéá\n"
                    "‚Ä¢ F1ÂàÜÊï∞ÔºöÁªºÂêàËÄÉËôëÁ≤æÁ°ÆÁéáÂíåÂè¨ÂõûÁéáÁöÑÊåáÊ†á\n"
                    "‚Ä¢ ËÆ≠ÁªÉÊó∂Èó¥(Áßí)ÔºöÊ®°ÂûãËÆ≠ÁªÉËÄóÊó∂\n\n"
                    "üí° ËßÇÂØüÊñπÊ≥ïÔºö\n"
                    "1. Êü•ÁúãÊ®°ÂûãÂú®Â§öÊ¨°Ëø≠‰ª£‰∏≠ÁöÑÁ®≥ÂÆöÊÄß\n"
                    "2. ÊØîËæÉ‰∏çÂêåÊ®°ÂûãÂú®Âêå‰∏ÄËø≠‰ª£‰∏≠ÁöÑË°®Áé∞\n"
                    "3. ËßÇÂØüËá™‰øÆÂ§çÂäüËÉΩÊòØÂê¶ÊèêÈ´ò‰∫ÜÊÄßËÉΩ")
                explanation_cell.alignment = Alignment(wrap_text=True, vertical='center')
                explanation_cell.font = Font(bold=True, size=11)
                explanation_cell.fill = PatternFill(start_color="E6FAE6", end_color="E6FAE6", fill_type="solid")
                
                # Write headers (starting from row 4)
                headers = ["Ëø≠‰ª£Ê¨°Êï∞", "Ê®°ÂûãÂêçÁß∞", "ÂáÜÁ°ÆÁéá", "F1ÂàÜÊï∞", "ËÆ≠ÁªÉÊó∂Èó¥(Áßí)"]
                for col, header in enumerate(headers, 1):
                    cell = ws2.cell(row=4, column=col, value=header)
                    cell.font = Font(bold=True)
                    cell.fill = PatternFill(start_color="CCCCCC", end_color="CCCCCC", fill_type="solid")
                    cell.alignment = Alignment(horizontal='center')
                
                # Write data (starting from row 5)
                row = 5
                for iteration in iteration_data:
                    iteration_num = iteration.get('iteration', 0)
                    model_performance = iteration.get('model_performance', {})
                    
                    for model_name, metrics in model_performance.items():
                        ws2.cell(row=row, column=1, value=iteration_num)
                        ws2.cell(row=row, column=2, value=model_name)
                        ws2.cell(row=row, column=3, value=metrics.get('accuracy', 0))
                        ws2.cell(row=row, column=4, value=metrics.get('f1_score', 0))
                        ws2.cell(row=row, column=5, value=metrics.get('training_time', 0))
                        row += 1
                
                # Adjust column widths
                for column in ws2.columns:
                    max_length = 0
                    column_letter = get_column_letter(column[0].column)
                    for cell in column:
                        try:
                            if cell.value and len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 30)
                    ws2.column_dimensions[column_letter].width = adjusted_width
                
                # Format numbers
                for row in ws2.iter_rows(min_row=5, max_row=row-1, min_col=3, max_col=5):
                    for cell in row:
                        if isinstance(cell.value, (int, float)):
                            cell.number_format = '0.00%' if cell.column <= 4 else '0.00'
            else:
                print(f"    ‚ö†Ô∏è Iteration results file {iteration_file} not found")
            
            # 3. Model Comparison Sheet with Explanation
            ws3 = wb.create_sheet("Ê®°ÂûãÂØπÊØî")
            
            # Analyze best models
            best_models = {}
            if os.path.exists(history_file):
                # Find best performance for each model
                model_best = {}
                for record in history_data:
                    model_name = record.get('model_name')
                    accuracy = record.get('accuracy', 0)
                    
                    if model_name not in model_best or accuracy > model_best[model_name]['accuracy']:
                        model_best[model_name] = {
                            'accuracy': accuracy,
                            'f1_score': record.get('f1_score', 0),
                            'iteration': record.get('iteration', 0),
                            'training_time': record.get('training_time', 0)
                        }
                
                # Add explanation at the top
                ws3.merge_cells('A1:F2')
                explanation_cell = ws3.cell(row=1, column=1, 
                    value="üèÜ Ê®°ÂûãÂØπÊØîË°®ËØ¥ÊòéÔºö\n"
                    "Ê≠§Ë°®Ê†ºÊ±áÊÄª‰∫ÜÊØè‰∏™Ê®°ÂûãÂú®Êï¥‰∏™ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÁöÑÊúÄ‰Ω≥Ë°®Áé∞„ÄÇ\n"
                    "‚Ä¢ Ê®°ÂûãÂêçÁß∞Ôºö‰ΩøÁî®ÁöÑÊú∫Âô®Â≠¶‰π†ÁÆóÊ≥ï\n"
                    "‚Ä¢ ÊúÄ‰Ω≥ÂáÜÁ°ÆÁéáÔºöÊ®°ÂûãÂú®Êï¥‰∏™ËÆ≠ÁªÉ‰∏≠ËææÂà∞ÁöÑÊúÄÈ´òÂáÜÁ°ÆÁéá\n"
                    "‚Ä¢ ÊúÄ‰Ω≥F1ÂàÜÊï∞ÔºöÊ®°ÂûãÂú®Êï¥‰∏™ËÆ≠ÁªÉ‰∏≠ËææÂà∞ÁöÑÊúÄÈ´òF1ÂàÜÊï∞\n"
                    "‚Ä¢ ËææÂà∞ËΩÆÊ¨°ÔºöËææÂà∞ÊúÄ‰Ω≥ÊÄßËÉΩÁöÑËÆ≠ÁªÉËΩÆÊ¨°\n"
                    "‚Ä¢ ËÆ≠ÁªÉÊó∂Èó¥(Áßí)ÔºöËææÂà∞ÊúÄ‰Ω≥ÊÄßËÉΩÊó∂ÁöÑËÆ≠ÁªÉËÄóÊó∂\n"
                    "‚Ä¢ ÊÄßËÉΩÊéíÂêçÔºöÊ†πÊçÆÊúÄ‰Ω≥ÂáÜÁ°ÆÁéá‰ªéÈ´òÂà∞‰ΩéÊéíÂêç\n\n"
                    "üí° ÂÜ≥Á≠ñÂª∫ËÆÆÔºö\n"
                    "1. ÊéíÂêçÁ¨¨1ÁöÑÊ®°ÂûãÊòØÊï¥‰ΩìÊúÄ‰Ω≥ÈÄâÊã©\n"
                    "2. ËÄÉËôëÂáÜÁ°ÆÁéáÂíåËÆ≠ÁªÉÊó∂Èó¥ÁöÑÂπ≥Ë°°\n"
                    "3. Â¶ÇÊûúÂáÜÁ°ÆÁéáÁõ∏ËøëÔºåÈÄâÊã©ËÆ≠ÁªÉÊó∂Èó¥Êõ¥Áü≠ÁöÑÊ®°Âûã")
                explanation_cell.alignment = Alignment(wrap_text=True, vertical='center')
                explanation_cell.font = Font(bold=True, size=11)
                explanation_cell.fill = PatternFill(start_color="FAE6E6", end_color="FAE6E6", fill_type="solid")
                
                # Write headers (starting from row 4)
                headers = ["Ê®°ÂûãÂêçÁß∞", "ÊúÄ‰Ω≥ÂáÜÁ°ÆÁéá", "ÊúÄ‰Ω≥F1ÂàÜÊï∞", "ËææÂà∞ËΩÆÊ¨°", "ËÆ≠ÁªÉÊó∂Èó¥(Áßí)", "ÊÄßËÉΩÊéíÂêç"]
                for col, header in enumerate(headers, 1):
                    cell = ws3.cell(row=4, column=col, value=header)
                    cell.font = Font(bold=True)
                    cell.fill = PatternFill(start_color="CCCCCC", end_color="CCCCCC", fill_type="solid")
                    cell.alignment = Alignment(horizontal='center')
                
                # Sort by accuracy
                sorted_models = sorted(model_best.items(), key=lambda x: x[1]['accuracy'], reverse=True)
                
                row = 5
                for rank, (model_name, metrics) in enumerate(sorted_models, 1):
                    ws3.cell(row=row, column=1, value=model_name)
                    ws3.cell(row=row, column=2, value=metrics['accuracy'])
                    ws3.cell(row=row, column=3, value=metrics['f1_score'])
                    ws3.cell(row=row, column=4, value=metrics['iteration'])
                    ws3.cell(row=row, column=5, value=metrics['training_time'])
                    ws3.cell(row=row, column=6, value=rank)
                    
                    # Add special marking for first place
                    if rank == 1:
                        for col in range(1, 7):
                            cell = ws3.cell(row=row, column=col)
                            cell.fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")
                            cell.font = Font(bold=True)
                    
                    row += 1
                
                # Adjust column widths
                for column in ws3.columns:
                    max_length = 0
                    column_letter = get_column_letter(column[0].column)
                    for cell in column:
                        try:
                            if cell.value and len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 25)
                    ws3.column_dimensions[column_letter].width = adjusted_width
                
                # Format numbers
                for row in ws3.iter_rows(min_row=5, max_row=row-1, min_col=2, max_col=3):
                    for cell in row:
                        if isinstance(cell.value, (int, float)):
                            cell.number_format = '0.00%'
            
            # 4. ROC/AUC Summary Sheet with Explanation
            ws4 = wb.create_sheet("ROC_AUCÊÄªÁªì")
            
            # Read ROC results
            roc_summary_file = os.path.join("roc_analysis", "ROC_AUC_Summary.json")
            if os.path.exists(roc_summary_file):
                with open(roc_summary_file, 'r', encoding='utf-8') as f:
                    roc_data = json.load(f)
                
                # Add explanation at the top
                ws4.merge_cells('A1:D2')
                explanation_cell = ws4.cell(row=1, column=1, 
                    value="üìä ROC/AUCÊÄªÁªìË°®ËØ¥ÊòéÔºö\n"
                    "Ê≠§Ë°®Ê†ºÊ±áÊÄª‰∫ÜÊØè‰∏™Ê®°ÂûãÁöÑROCÊõ≤Á∫øÂàÜÊûêÁªìÊûú„ÄÇ\n"
                    "‚Ä¢ Ê®°ÂûãÂêçÁß∞Ôºö‰ΩøÁî®ÁöÑÊú∫Âô®Â≠¶‰π†ÁÆóÊ≥ï\n"
                    "‚Ä¢ MicroÂπ≥ÂùáAUCÔºöËÄÉËôëÊâÄÊúâÊ†∑Êú¨ÁöÑÂπ≥ÂùáAUCÂÄº\n"
                    "‚Ä¢ MacroÂπ≥ÂùáAUCÔºöËÄÉËôëÊâÄÊúâÁ±ªÂà´ÁöÑÂπ≥ÂùáAUCÂÄº\n"
                    "‚Ä¢ Á±ªÂà´AUC(Âπ≥Âùá)ÔºöÂêÑËØäÊñ≠Á±ªÂà´AUCÂÄºÁöÑÂπ≥ÂùáÂÄº\n\n"
                    "üìà AUCÂÄºËß£ÈáäÔºö\n"
                    "‚Ä¢ 0.9-1.0ÔºöÊûÅÂ•ΩÁöÑÂå∫ÂàÜËÉΩÂäõ\n"
                    "‚Ä¢ 0.8-0.9ÔºöËâØÂ•ΩÁöÑÂå∫ÂàÜËÉΩÂäõ\n"
                    "‚Ä¢ 0.7-0.8Ôºö‰∏≠Á≠âÂå∫ÂàÜËÉΩÂäõ\n"
                    "‚Ä¢ 0.6-0.7ÔºöËæÉÂ∑ÆÁöÑÂå∫ÂàÜËÉΩÂäõ\n"
                    "‚Ä¢ <0.6ÔºöÊó†Âå∫ÂàÜËÉΩÂäõ")
                explanation_cell.alignment = Alignment(wrap_text=True, vertical='center')
                explanation_cell.font = Font(bold=True, size=11)
                explanation_cell.fill = PatternFill(start_color="E6FAFA", end_color="E6FAFA", fill_type="solid")
                
                # Write headers (starting from row 4)
                headers = ["Ê®°ÂûãÂêçÁß∞", "MicroÂπ≥ÂùáAUC", "MacroÂπ≥ÂùáAUC", "Á±ªÂà´AUC(Âπ≥Âùá)"]
                for col, header in enumerate(headers, 1):
                    cell = ws4.cell(row=4, column=col, value=header)
                    cell.font = Font(bold=True)
                    cell.fill = PatternFill(start_color="CCCCCC", end_color="CCCCCC", fill_type="solid")
                    cell.alignment = Alignment(horizontal='center')
                
                # Write data (starting from row 5)
                row = 5
                for model_name, result in roc_data.items():
                    if 'error' not in result:
                        ws4.cell(row=row, column=1, value=model_name)
                        ws4.cell(row=row, column=2, value=result.get('micro_auc', 0))
                        ws4.cell(row=row, column=3, value=result.get('macro_auc', 0))
                        
                        # Calculate average class AUC
                        class_aucs = result.get('class_auc', {})
                        if class_aucs:
                            # Extract numeric AUC values (skip 'micro' and 'macro')
                            auc_values = [v for k, v in class_aucs.items() if k not in ['micro', 'macro']]
                            if auc_values:
                                avg_class_auc = np.mean(auc_values)
                                ws4.cell(row=row, column=4, value=avg_class_auc)
                        
                        row += 1
                
                # Adjust column widths
                for column in ws4.columns:
                    max_length = 0
                    column_letter = get_column_letter(column[0].column)
                    for cell in column:
                        try:
                            if cell.value and len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 25)
                    ws4.column_dimensions[column_letter].width = adjusted_width
                
                # Format numbers
                for row in ws4.iter_rows(min_row=5, max_row=row-1, min_col=2, max_col=4):
                    for cell in row:
                        if isinstance(cell.value, (int, float)):
                            cell.number_format = '0.000'
            else:
                print(f"    ‚ö†Ô∏è ROC summary file {roc_summary_file} not found")
            
            # 5. Diagnosis Category Mapping Sheet
            ws5 = wb.create_sheet("ËØäÊñ≠Á±ªÂà´Êò†Â∞Ñ")
            
            # Add explanation at the top
            ws5.merge_cells('A1:C3')
            explanation_cell = ws5.cell(row=1, column=1, 
                value="üî§ ËØäÊñ≠Á±ªÂà´Êò†Â∞ÑË°®ËØ¥ÊòéÔºö\n"
                "Ê≠§Ë°®Ê†ºÊòæÁ§∫‰∫ÜËØäÊñ≠Á±ªÂà´ÁºñÁ†Å‰∏éËã±ÊñáÂêçÁß∞ÁöÑÂØπÂ∫îÂÖ≥Á≥ª„ÄÇ\n"
                "‚Ä¢ Á±ªÂà´ÁºñÁ†ÅÔºöÊú∫Âô®Â≠¶‰π†Ê®°Âûã‰∏≠‰ΩøÁî®ÁöÑÊï∞Â≠óÁºñÁ†Å\n"
                "‚Ä¢ Ëã±ÊñáÂêçÁß∞ÔºöËØäÊñ≠Á±ªÂà´ÁöÑËã±ÊñáÂêçÁß∞\n"
                "‚Ä¢ ‰∏≠ÊñáËß£ÈáäÔºöËØäÊñ≠Á±ªÂà´ÁöÑ‰∏≠ÊñáÂê´‰πâ\n\n"
                "üí° ‰ΩøÁî®ËØ¥ÊòéÔºö\n"
                "Âú®ÂàÜÊûêROCÊõ≤Á∫øÂíåÊ®°ÂûãÈ¢ÑÊµãÁªìÊûúÊó∂ÔºåÂèÇËÄÉÊ≠§Êò†Â∞ÑË°®ÁêÜËß£ÂêÑÁ±ªÂà´ÁöÑÂê´‰πâ„ÄÇ")
            explanation_cell.alignment = Alignment(wrap_text=True, vertical='center')
            explanation_cell.font = Font(bold=True, size=11)
            explanation_cell.fill = PatternFill(start_color="FAF0E6", end_color="FAF0E6", fill_type="solid")
            
            # Write headers (starting from row 5)
            headers = ["Á±ªÂà´ÁºñÁ†Å", "Ëã±ÊñáÂêçÁß∞", "‰∏≠ÊñáËß£Èáä"]
            for col, header in enumerate(headers, 1):
                cell = ws5.cell(row=5, column=col, value=header)
                cell.font = Font(bold=True)
                cell.fill = PatternFill(start_color="CCCCCC", end_color="CCCCCC", fill_type="solid")
                cell.alignment = Alignment(horizontal='center')
            
            # Define diagnosis category mapping with explanations
            diagnosis_mapping = [
                (0, "Hyperopia", "ËøúËßÜ"),
                (1, "Emmetropia", "Ê≠£ËßÜ"),
                (2, "Pre-myopia", "ËøëËßÜÂâçÊúü"),
                (3, "Mild Myopia", "ËΩªÂ∫¶ËøëËßÜ"),
                (4, "Mild Myopia (Long Axial)", "ËΩªÂ∫¶ËøëËßÜÔºàÁúºËΩ¥ÂÅèÈïøÔºâ"),
                (5, "Moderate Myopia", "‰∏≠Â∫¶ËøëËßÜ"),
                (6, "High Myopia", "È´òÂ∫¶ËøëËßÜ"),
                (7, "Unclassified", "Êú™ÂàÜÁ±ª"),
                (8, "Diagnosis Failed", "ËØäÊñ≠Â§±Ë¥•")
            ]
            
            # Write data (starting from row 6)
            row = 6
            for code, en_name, cn_explanation in diagnosis_mapping:
                ws5.cell(row=row, column=1, value=code)
                ws5.cell(row=row, column=2, value=en_name)
                ws5.cell(row=row, column=3, value=cn_explanation)
                row += 1
            
            # Adjust column widths
            for column in ws5.columns:
                max_length = 0
                column_letter = get_column_letter(column[0].column)
                for cell in column:
                    try:
                        if cell.value and len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 30)
                ws5.column_dimensions[column_letter].width = adjusted_width
            
            # Save Excel file
            wb.save(output_file)
            print(f"  ‚úÖ Excel training report generated: {output_file}")
            
            return True
            
        except Exception as e:
            print(f"  ‚ùå Failed to generate Excel report: {e}")
            traceback.print_exc()
            return False

class ContinuousLearningPipeline:
    """Continuous Learning Pipeline"""
    
    def __init__(self, excel_path: str, max_iterations: int = 10):
        self.excel_path = excel_path
        self.max_iterations = max_iterations
        self.data_processor = EnhancedDataProcessor()
        self.multi_model = MultiModelTraining()
        self.iteration_results = []
        self.full_data = None
        
    # ‰øÆÊîπ ContinuousLearningPipeline ‰∏≠ÁöÑ run_continuous_training ÊñπÊ≥ïÔºö

def run_continuous_training(self):
    """ËøêË°åÊåÅÁª≠ËÆ≠ÁªÉ"""
    print("="*60)
    print("ËøëËßÜËØäÊñ≠Ê®°ÂûãÊåÅÁª≠Â≠¶‰π†ÁÆ°ÈÅì")
    print(f"ÊúÄÂ§ßËø≠‰ª£Ê¨°Êï∞: {self.max_iterations}")
    print("="*60)
    
    try:
        # 1. Âä†ËΩΩÂíåÈ¢ÑÂ§ÑÁêÜÊï∞ÊçÆ
        print("\n[Èò∂ÊÆµ1] Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ...")
        df = self.data_processor.load_data(self.excel_path)
        processed_data = self.data_processor.process_data(df)
        
        # 2. ÁîüÊàêÊÇ£ËÄÖID
        processed_data = self.data_processor.generate_patient_ids(processed_data)
        
        # 3. ËøêË°åËßÑÂàôËØäÊñ≠
        labeled_data = self.data_processor.run_rule_diagnosis(processed_data)
        self.full_data = labeled_data
        
        # 4. ÂàùÂßãÂåñÊ®°Âûã
        print("\n[Èò∂ÊÆµ2] ÂàùÂßãÂåñÊú∫Âô®Â≠¶‰π†Ê®°Âûã...")
        self.multi_model.initialize_models()
        
        # 5. Â§öËΩÆËÆ≠ÁªÉ
        print("\n[Èò∂ÊÆµ3] Â§öËΩÆËÆ≠ÁªÉ...")
        for iteration in range(1, self.max_iterations + 1):
            print(f"\n{'='*50}")
            print(f"Á¨¨ {iteration}/{self.max_iterations} ËΩÆËÆ≠ÁªÉ")
            print(f"{'='*50}")
            
            # ‰ΩøÁî®‰∏çÂêåÁöÑÈöèÊú∫ÁßçÂ≠êÁ°Æ‰øùÊØèÊ¨°Êï∞ÊçÆÂàÜÂâ≤ÈÉΩ‰∏çÂêå
            # ‰ΩøÁî®Êó∂Èó¥Êà≥Á°Æ‰øùÁúüÊ≠£ÁöÑÈöèÊú∫ÊÄß
            current_time_seed = int(time.time() * 1000) % 10000
            random_seed = current_time_seed + iteration * 100
            
            print(f"  ‰ΩøÁî®ÈöèÊú∫ÁßçÂ≠ê: {random_seed}")
            
            try:
                # ‰ΩøÁî®‰∏çÂêåÁöÑÈöèÊú∫ÁßçÂ≠êÂàÜÂâ≤Êï∞ÊçÆ
                train_data, val_data, test_data = self.data_processor.split_data_by_patient_id(
                    self.full_data,
                    random_state=random_seed  # ‰º†ÂÖ•ÈöèÊú∫ÁßçÂ≠ê
                )
                
                # Ê£ÄÊü•Êï∞ÊçÆÊòØÂê¶ÊúâÂèòÂåñ
                if iteration > 1:
                    self._check_data_variation(train_data, val_data, test_data, iteration)
                
                # ËÆ≠ÁªÉÊâÄÊúâÊ®°Âûã
                results = self.multi_model.train_and_evaluate_all(
                    train_data, val_data, test_data, iteration
                )
                
                # ÂØπÊØîÊ®°ÂûãÊÄßËÉΩ
                comparison_df = self.multi_model.compare_models(results)
                
                # ‰øùÂ≠òÊú¨ËΩÆÁªìÊûú
                self.iteration_results.append({
                    'iteration': iteration,
                    'results': results,
                    'comparison': comparison_df.to_dict(),
                    'random_seed': random_seed  # ‰øùÂ≠òÈöèÊú∫ÁßçÂ≠ê
                })
                
                # ÈõÜÊàêÈ¢ÑÊµã
                ensemble_pred, ensemble_proba = self.multi_model.ensemble_predict(
                    results, self.multi_model.prepare_features(test_data)
                )
                
                if ensemble_pred is not None:
                    ensemble_accuracy = accuracy_score(test_data['ËØäÊñ≠ÁºñÁ†Å'].values, ensemble_pred)
                    print(f"  ü§ù ÈõÜÊàêÊ®°ÂûãÂáÜÁ°ÆÁéá: {ensemble_accuracy:.2%}")
                
                # ‰øùÂ≠òÊï∞ÊçÆÈõÜÔºàÁ¨¨‰∏ÄÊ¨°Ëø≠‰ª£Ôºâ
                if iteration == 1:
                    self.data_processor.save_datasets(train_data, val_data, test_data)
                
                # Á≠âÂæÖ‰∏Ä‰∏ãÔºàÁ°Æ‰øùÈöèÊú∫ÁßçÂ≠êÂèòÂåñÔºâ
                if iteration < self.max_iterations:
                    wait_time = 0.1  # Áü≠ÊöÇÁ≠âÂæÖ‰ª•Á°Æ‰øùÊó∂Èó¥Êà≥‰∏çÂêå
                    time.sleep(wait_time)
                    
            except Exception as e:
                print(f"  Á¨¨{iteration}ËΩÆËÆ≠ÁªÉÂ§±Ë¥•: {e}")
                continue
        
        # 6. ‰øùÂ≠òÊâÄÊúâÊ®°ÂûãÂíåÁªìÊûú
        print("\n" + "="*60)
        print("[Èò∂ÊÆµ4] ‰øùÂ≠òÊ®°ÂûãÂíåËÆ≠ÁªÉÁªìÊûú...")
        self.multi_model.save_all_models()
        
        # ‰øùÂ≠òËø≠‰ª£ÁªìÊûú
        self.save_iteration_results()
        
        # ÁîüÊàêÊúÄÁªàÊä•Âëä
        self.generate_final_report()
        
        print("\n" + "="*60)
        print("‚úÖ ÊåÅÁª≠Â≠¶‰π†ËÆ≠ÁªÉÂÆåÊàêÔºÅ")
        print("="*60)
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå ÊµÅÁ®ãÊâßË°åÂ§±Ë¥•: {e}")
        traceback.print_exc()
        return False

def _check_data_variation(self, train_data, val_data, test_data, iteration):
    """Ê£ÄÊü•Êï∞ÊçÆÂèòÂåñ"""
    if iteration > 1:
        # ÁÆÄÂçïÊ£ÄÊü•ÔºöÊØîËæÉÊ†∑Êú¨Êï∞ÈáèÊòØÂê¶ÊúâÂèòÂåñ
        prev_train_size = getattr(self, '_prev_train_size', 0)
        prev_val_size = getattr(self, '_prev_val_size', 0)
        prev_test_size = getattr(self, '_prev_test_size', 0)
        
        train_change = abs(len(train_data) - prev_train_size)
        val_change = abs(len(val_data) - prev_val_size)
        test_change = abs(len(test_data) - prev_test_size)
        
        if train_change + val_change + test_change > 0:
            print(f"  Êï∞ÊçÆÂàÜÂâ≤ÂèòÂåñ: ËÆ≠ÁªÉÈõÜÂèòÂåñ{train_change}‰∏™Ê†∑Êú¨")
        else:
            print(f"  ‚ö†Ô∏è Êï∞ÊçÆÂàÜÂâ≤Ê≤°ÊúâÂèòÂåñÔºÅ")
            print(f"  ÂèØËÉΩÈúÄË¶ÅÊ£ÄÊü•Êï∞ÊçÆÈáèÊàñÈöèÊú∫ÁßçÂ≠êÁîüÊàê")
        
        # ‰øùÂ≠òÂΩìÂâçÂ§ßÂ∞è‰æõ‰∏ãÊ¨°ÊØîËæÉ
        self._prev_train_size = len(train_data)
        self._prev_val_size = len(val_data)
        self._prev_test_size = len(test_data)
        
    def save_iteration_results(self):
        """Save iteration results"""
        output_file = "iteration_results.json"
        
        # Simplify results for saving
        simplified_results = []
        for iteration_data in self.iteration_results:
            simple_iteration = {
                'iteration': iteration_data['iteration'],
                'model_performance': {}
            }
            
            if 'results' in iteration_data:
                for model_name, result in iteration_data['results'].items():
                    if 'metrics' in result:
                        simple_iteration['model_performance'][model_name] = {
                            'accuracy': result['metrics']['accuracy'],
                            'f1_score': result['metrics']['f1_score'],
                            'training_time': result['metrics']['training_time']
                        }
            
            simplified_results.append(simple_iteration)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(simplified_results, f, ensure_ascii=False, indent=2)
        
        print(f"  Iteration results saved to: {output_file}")
    
    def generate_final_report(self):
        """Generate final report"""
        print("\n" + "="*60)
        print("Final Training Report")
        print("="*60)
        
        # Get best models
        best_models = self.multi_model.performance_tracker.get_best_models()
        
        print("\nüèÜ Best performance for each model:")
        for model_name, best_info in best_models.items():
            metrics = best_info['metrics']
            print(f"\n  {model_name}:")
            print(f"    Iteration: {best_info['iteration']}")
            print(f"    Accuracy: {metrics['accuracy']:.2%}")
            print(f"    F1 Score: {metrics['f1_score']:.2%}")
            print(f"    Validation Accuracy: {metrics.get('val_accuracy', 0):.2%}")
            print(f"    Training Time: {metrics['training_time']:.2f}s")
            print(f"    Time: {best_info['timestamp']}")
        
        # Summary
        print("\nüìä Training Summary:")
        print(f"  Total iterations: {len(self.iteration_results)}")
        print(f"  Number of trained models: {len(self.multi_model.models)}")
        print(f"  Total data volume: {len(self.full_data) if self.full_data is not None else 0}")
        
        # Generate recommendations
        print("\nüí° Recommendations:")
        if best_models:
            best_model_name = max(best_models.keys(), 
                                key=lambda x: best_models[x]['metrics']['accuracy'])
            print(f"  Recommended model: {best_model_name}")
            print(f"  Expected accuracy: {best_models[best_model_name]['metrics']['accuracy']:.2%}")
        
        print("\nüìÅ Generated files:")
        print("  1. saved_models/ - All trained models")
        print("  2. model_history.json - Model training history")
        print("  3. iteration_results.json - Iteration results")
        print("  4. training_set.xlsx, validation_set.xlsx, test_set.xlsx - Dataset splits")

class EnhancedContinuousLearningPipeline(ContinuousLearningPipeline):
    """Enhanced Continuous Learning Pipeline (with testing and reporting)"""
    
    def __init__(self, excel_path: str, max_iterations: int = 10):
        super().__init__(excel_path, max_iterations)
        self.roc_results = {}
        self.final_test_results = {}
        
    def run_continuous_training(self):
        """Run enhanced training pipeline"""
        # Call parent class training method
        success = super().run_continuous_training()
        
        if success:
            # Add extra functionality
            self.generate_excel_reports()
            self.test_real_cases()
            
        return success
    
    def generate_excel_reports(self):
        """Generate Excel reports"""
        print("\n" + "="*60)
        print("[Phase 5] Generating Excel reports and ROC analysis")
        print("="*60)
        
        # Generate Excel report
        ExcelReportGenerator.generate_training_report(
            os.path.join("saved_models", "model_history.json"),
            "iteration_results.json",
            "ËÆ≠ÁªÉÊä•Âëä.xlsx"
        )
        
        # Analyze ROC curves (using last iteration results)
        if hasattr(self.multi_model, 'analyze_roc_for_all_models') and self.iteration_results:
            last_iteration = self.iteration_results[-1]
            if 'results' in last_iteration:
                self.roc_results = self.multi_model.analyze_roc_for_all_models(
                    last_iteration['results'],
                    label_encoder=self.data_processor.label_encoder,
                    output_dir="roc_analysis"
                )
    
    def test_real_cases(self):
        """Test real cases"""
        print("\n" + "="*60)
        print("[Phase 6] Real Case Testing")
        print("="*60)
        
        # Select some cases from test set for testing
        if hasattr(self, 'full_data') and self.full_data is not None:
            # Find patients in test set
            test_patients = self.full_data[self.full_data['patient_id'].str.contains('TEST', na=False)]
            if len(test_patients) == 0:
                # If no TEST label, randomly select
                test_patients = self.full_data.sample(min(5, len(self.full_data)))
            
            print(f"  Selected {len(test_patients)} cases for testing:")
            
            for idx, patient in test_patients.iterrows():
                self.test_single_case(patient)
                print("-" * 50)
        
        # Interactive testing
        self.interactive_testing()
    
    def test_single_case(self, patient_data):
        """Test single case"""
        try:
            print(f"\n  Testing case {patient_data['ÂßìÂêç']} ({patient_data['ÁúºÂà´']}):")
            print(f"    Age: {patient_data['Âπ¥ÈæÑ']} years")
            print(f"    Gender: {patient_data['ÊÄßÂà´']}")
            print(f"    Vision: {patient_data['ËßÜÂäõ']}")
            print(f"    Refraction: {patient_data['ÂéüÂßã_È™åÂÖâ']}D")
            print(f"    Axial length: {patient_data['ÁúºËΩ¥']}mm")
            print(f"    Axial ratio: {patient_data['ËΩ¥ÁéáÊØî']}")
            print(f"    Corneal curvature: {patient_data['ËßíËÜúÊõ≤Áéá']}D")
            
            # Use rule-based system for diagnosis
            rule_diagnosis = self.data_processor.rule_classifier.diagnose({
                'age': patient_data['Âπ¥ÈæÑ'],
                'se': patient_data['È™åÂÖâ'],
                'al': patient_data['ÁúºËΩ¥'],
                'corneal_curv': patient_data['ËßíËÜúÊõ≤Áéá'],
                'va': patient_data['ËßÜÂäõ'],
                'is_cycloplegic': False
            })
            
            print(f"    Rule-based diagnosis: {rule_diagnosis['stage']}")
            
            # Use machine learning models for diagnosis
            ml_predictions = {}
            
            # Prepare features
            patient_features = self.multi_model.prepare_features(pd.DataFrame([patient_data]))
            
            # Predict using each model
            for model_name, model_wrapper in self.multi_model.models.items():
                try:
                    model = model_wrapper.base_model
                    prediction = model.predict(patient_features)[0]
                    
                    # If probabilities available, also display
                    if hasattr(model, 'predict_proba'):
                        proba = model.predict_proba(patient_features)[0]
                        max_prob = np.max(proba)
                        ml_predictions[model_name] = {
                            'prediction': prediction,
                            'probability': max_prob,
                            'all_probs': proba.tolist()
                        }
                    else:
                        ml_predictions[model_name] = {
                            'prediction': prediction,
                            'probability': None
                        }
                    
                except Exception as e:
                    print(f"    ‚ùå {model_name} prediction failed: {e}")
            
            # Display machine learning prediction results
            print(f"    Machine learning diagnosis:")
            
            for model_name, pred_info in ml_predictions.items():
                if 'prediction' in pred_info:
                    # Decode prediction result
                    diagnosis_label = "Unknown"
                    try:
                        if pred_info['prediction'] in self.data_processor.label_encoder.classes_:
                            diagnosis_label = self.data_processor.label_encoder.inverse_transform(
                                [pred_info['prediction']]
                            )[0]
                    except:
                        pass
                    
                    prob_text = f" (confidence: {pred_info['probability']:.2%})" if pred_info['probability'] else ""
                    print(f"      {model_name}: {diagnosis_label}{prob_text}")
            
            # True diagnosis (if available)
            if 'ËØäÊñ≠ÁªìÊûú' in patient_data:
                print(f"    True diagnosis: {patient_data['ËØäÊñ≠ÁªìÊûú']}")
                
                # Check if matches
                rule_match = rule_diagnosis['stage'] == patient_data['ËØäÊñ≠ÁªìÊûú']
                print(f"    Rule-based diagnosis match: {'‚úÖ' if rule_match else '‚ùå'}")
                
                # Check machine learning diagnosis match
                for model_name, pred_info in ml_predictions.items():
                    if 'prediction' in pred_info:
                        ml_diagnosis = "Unknown"
                        try:
                            if pred_info['prediction'] in self.data_processor.label_encoder.classes_:
                                ml_diagnosis = self.data_processor.label_encoder.inverse_transform(
                                    [pred_info['prediction']]
                                )[0]
                        except:
                            pass
                        
                        ml_match = ml_diagnosis == patient_data['ËØäÊñ≠ÁªìÊûú']
                        print(f"    {model_name} match: {'‚úÖ' if ml_match else '‚ùå'}")
            
            return {
                'patient_info': {
                    'ÂßìÂêç': patient_data['ÂßìÂêç'],
                    'Âπ¥ÈæÑ': patient_data['Âπ¥ÈæÑ'],
                    'ÁúºÂà´': patient_data['ÁúºÂà´']
                },
                'rule_diagnosis': rule_diagnosis['stage'],
                'ml_predictions': ml_predictions,
                'true_diagnosis': patient_data.get('ËØäÊñ≠ÁªìÊûú', 'Unknown')
            }
            
        except Exception as e:
            print(f"    Case testing failed: {e}")
            return None
    
    def interactive_testing(self):
        """Interactive testing"""
        print("\n  Interactive testing:")
        print("  1. Use random cases from test data")
        print("  2. Manually enter case information")
        print("  3. Batch testing")
        print("  4. Exit testing")
        
        try:
            choice = input("  Please choose testing method (1-4): ")
            
            if choice == '1':
                self.test_random_cases()
            elif choice == '2':
                self.test_manual_input()
            elif choice == '3':
                self.test_batch_cases()
            elif choice == '4':
                print("  Exiting testing")
            else:
                print("  Invalid choice")
                
        except Exception as e:
            print(f"  Interactive testing failed: {e}")
    
    def test_random_cases(self, n_cases=3):
        """Randomly test cases"""
        if hasattr(self, 'full_data') and self.full_data is not None:
            random_cases = self.full_data.sample(min(n_cases, len(self.full_data)))
            
            print(f"\n  Randomly testing {len(random_cases)} cases:")
            
            for idx, case in random_cases.iterrows():
                result = self.test_single_case(case)
                self.final_test_results[f"random_{idx}"] = result
                print("-" * 50)
    
    def test_manual_input(self):
        """Manually input case information for testing"""
        print("\n  Manual case information input:")
        
        try:
            name = input("  Name: ") or "Test Patient"
            age = int(input("  Age: ") or "8")
            gender = input("  Gender (Áî∑/Â•≥): ") or "Áî∑"
            eye = input("  Eye (Â∑¶Áúº/Âè≥Áúº): ") or "Âè≥Áúº"
            vision = float(input("  Vision: ") or "0.8")
            refraction = float(input("  Refraction (D): ") or "-1.5")
            axial_length = float(input("  Axial length (mm): ") or "24.5")
            axial_ratio = float(input("  Axial ratio: ") or "3.2")
            corneal_curv = float(input("  Corneal curvature (D): ") or "43.2")
            
            # Create simulated patient data
            patient_data = {
                'ÂßìÂêç': name,
                'Âπ¥ÈæÑ': age,
                'ÊÄßÂà´': gender,
                'ÁúºÂà´': eye,
                'ËßÜÂäõ': vision,
                'È™åÂÖâ': refraction,
                'ÁúºËΩ¥': axial_length,
                'ËΩ¥ÁéáÊØî': axial_ratio,
                'ËßíËÜúÊõ≤Áéá': corneal_curv,
                'ÂéüÂßã_Âπ¥ÈæÑ': str(age),
                'ÂéüÂßã_È™åÂÖâ': str(refraction),
                'patient_key': f"{name}_{1 if gender == 'Â•≥' else 0}_{age}",
                'patient_id': PatientIDGenerator.generate_patient_id({'ÂßìÂêç': name, 'ÊÄßÂà´': gender, 'Âπ¥ÈæÑ': age})
            }
            
            result = self.test_single_case(patient_data)
            self.final_test_results["manual_input"] = result
            
        except Exception as e:
            print(f"  Manual input failed: {e}")
    
    def test_batch_cases(self):
        """Batch testing (read from file)"""
        print("\n  Batch testing functionality will be implemented in future versions")
        print("  You can create CSV or Excel files containing multiple case information")

# Main program
if __name__ == "__main__":
    EXCEL_FILE = "ÂçöÂ£´Êï∞ÊçÆÊî∂ÈõÜÈÄÇÁî®.xlsx"  # Replace with your Excel path
    
    print("Myopia Diagnosis Continuous Learning System - Enhanced Version")
    print("="*60)
    print("Supported models: RandomForest, LogisticRegression, SVM, GradientBoosting" + (", XGBoost" if XGBOOST_AVAILABLE else ""))
    print("="*60)
    
    # Get number of iterations
    try:
        max_iterations = int(input("Please enter number of training iterations (default 5): ") or "5")
    except:
        max_iterations = 5
    
    # Run enhanced continuous learning pipeline
    pipeline = EnhancedContinuousLearningPipeline(EXCEL_FILE, max_iterations)
    success = pipeline.run_continuous_training()
    
    if success:
        print("\n" + "="*60)
        print("üéâ Enhanced training completed! All functions executed.")
        print("="*60)
        
        print("\nüìÅ Generated files and reports:")
        print("  1. saved_models/ - All trained models")
        print("  2. ËÆ≠ÁªÉÊä•Âëä.xlsx - Excel training report (with history, iterations, comparison)")
        print("  3. roc_analysis/ - ROC curves and AUC analysis results")
        print("  4. iteration_results.json - Iteration results")
        print("  5. ËÆ≠ÁªÉÈõÜ.xlsx, È™åËØÅÈõÜ.xlsx, ÊµãËØïÈõÜ.xlsx - Dataset splits")
        print("  6. model_history.json - Model training history")
        
        # Display best model
        if hasattr(pipeline.multi_model, 'performance_tracker'):
            best_models = pipeline.multi_model.performance_tracker.get_best_models()
            if best_models:
                print("\nüèÜ Best model summary:")
                best_model_name = max(best_models.keys(), 
                                    key=lambda x: best_models[x]['metrics']['accuracy'])
                print(f"  Recommended model: {best_model_name}")
                print(f"  Best accuracy: {best_models[best_model_name]['metrics']['accuracy']:.2%}")
                
        # ROC results summary
        if pipeline.roc_results:
            print("\nüìä ROC/AUC Summary:")
            for model_name, result in pipeline.roc_results.items():
                if 'error' not in result:
                    print(f"  {model_name}:")
                    print(f"    Macro-average AUC: {result.get('macro_auc', 0):.3f}")
                    print(f"    Micro-average AUC: {result.get('micro_auc', 0):.3f}")
    else:
        print("\n‚ùå Issues occurred during training, please check logs.")
